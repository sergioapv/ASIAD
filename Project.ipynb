{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Conv2D, MaxPool2D, Flatten, Input, Concatenate, Lambda, experimental\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from tensorflow import keras\n",
    "from PIL import Image, ImageOps, ImageDraw\n",
    "import os, glob\n",
    "import numpy as np\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Definir Tamanhos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grayscale = False\n",
    "g = 1\n",
    "s = 3\n",
    "\n",
    "if(grayscale == True):\n",
    "    g = 0\n",
    "    s = 1\n",
    "\n",
    "#Se fizer load em RGB (1)\n",
    "#Se fizer load em Grayscale (0)\n",
    "\n",
    "size = (300,300)\n",
    "input = (300,300,s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cria arrays com imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"D:/Tese/Projeto BALCAT/Imagens/NIST CARTS/NIST Cartridges/Train\"\n",
    "labels = os.listdir(train_path)\n",
    "\n",
    "def create_lists(type):\n",
    "    train_path = \"D:/Tese/Projeto BALCAT/Imagens/NIST CARTS/NIST Cartridges/Train\"\n",
    "    labels = os.listdir(train_path)\n",
    "\n",
    "    #Converte labels em labels binários \n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    bin_lab = lb.fit_transform(labels)\n",
    "    path = \"D:/Tese/Projeto BALCAT/Imagens/NIST CARTS/NIST Cartridges/\" + type\n",
    "\n",
    "    image_set = []\n",
    "    b_labels_set = []\n",
    "    i_labels_set = []\n",
    "\n",
    "    #Cria train_set e train_labels\n",
    "    for i,b in zip(labels, bin_lab): \n",
    "        class_path = path + \"/\" + i\n",
    "\n",
    "        carts = os.listdir(class_path)\n",
    "        for cart in carts:\n",
    "            img = cv2.imread(class_path +  \"/\" + cart, g)\n",
    "            img = cv2.resize(img, size, interpolation=cv2.INTER_CUBIC)\n",
    "            img = np.reshape(img, input)\n",
    "            image_set.append(img/255)\n",
    "            b_labels_set.append(b)\n",
    "            i_labels_set.append(i)\n",
    "\n",
    "    #Faz shuffle às listas de forma igual\n",
    "    c = list(zip(image_set, b_labels_set, i_labels_set))\n",
    "    random.shuffle(c)\n",
    "    image_set, b_labels_set, i_labels_set = zip(*c)\n",
    "    return np.array(image_set), np.array(b_labels_set), np.array(i_labels_set)\n",
    "\n",
    "\n",
    "#Train set e labels em forma de numpy array\n",
    "train_set, train_labels, s_train_labels = create_lists(\"Train\")\n",
    "\n",
    "val_set, val_labels, s_val_labels = create_lists(\"Validation\")\n",
    "\n",
    "test_set, test_labels, s_test_labels = create_lists(\"Test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cria DF para image Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"D:/Tese/Projeto BALCAT/Imagens/NIST CARTS/NIST Cartridges/Train\"\n",
    "labels = os.listdir(train_path)\n",
    "\n",
    "#Converte labels em labels binários \n",
    "lb = preprocessing.LabelBinarizer()\n",
    "bin_lab = lb.fit_transform(labels)\n",
    "\n",
    "\n",
    "df = pd.DataFrame([], columns=['path', 'label'])\n",
    "df.head()\n",
    "\n",
    "#Vai acrescentando filepath e label ao dataframe\n",
    "for i,b in zip(labels, bin_lab): \n",
    "    class_path = train_path + \"/\" + i\n",
    "    carts = os.listdir(class_path)\n",
    "    for cart in carts:\n",
    "        df_l = pd.DataFrame({\"path\": [class_path + \"/\" + cart], \"label\": [i]})\n",
    "        df = df.append(df_l, ignore_index=True)\n",
    "\n",
    "\n",
    "#shuffle dataframe\n",
    "df = pd.concat([df[:1], df[1:].sample(frac=1)]).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cria os geradors com data augmentation (comentado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_gen():\n",
    "    # Load the Images with a generator and Data Augmentation\n",
    "    train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    train_images = train_generator.flow(\n",
    "\n",
    "        x_col='path',\n",
    "        y_col='label',\n",
    "        target_size=size,\n",
    "        color_mode='grayscale',\n",
    "        class_mode='categorical',\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        seed=0,\n",
    "        subset='training',\n",
    "        # rotation_range=30, # Uncomment to use data augmentation\n",
    "        # zoom_range=0.15,\n",
    "        # width_shift_range=0.2,\n",
    "        # height_shift_range=0.2,\n",
    "        # shear_range=0.15,\n",
    "        # horizontal_flip=True,\n",
    "        # fill_mode=\"nearest\"\n",
    "    )\n",
    "\n",
    "    val_images = train_generator.flow_from_dataframe(\n",
    "        dataframe=df,\n",
    "        x_col='path',\n",
    "        y_col='label',\n",
    "        target_size=size,\n",
    "        color_mode='grayscale',\n",
    "        class_mode='categorical',\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        seed=0,\n",
    "        subset='validation'\n",
    "        # rotation_range=30, # Uncomment to use data augmentation\n",
    "        # zoom_range=0.15,\n",
    "        # width_shift_range=0.2,\n",
    "        # height_shift_range=0.2,\n",
    "        # shear_range=0.15,\n",
    "        # horizontal_flip=True,\n",
    "        # fill_mode=\"nearest\"\n",
    "    )\n",
    "\n",
    "    # test_images = test_generator.flow_from_dataframe(\n",
    "    #     dataframe=test_df,\n",
    "    #     x_col='Filepath',\n",
    "    #     y_col='Label',\n",
    "    #     target_size=(224, 224),\n",
    "    #     color_mode='rgb',\n",
    "    #     class_mode='categorical',\n",
    "    #     batch_size=32,\n",
    "    #     shuffle=False\n",
    "    # )\n",
    "    \n",
    "    return train_generator,train_images,val_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator,train_images,val_image = create_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(train_set[2]), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(val_image[0][0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criaçao de modelo normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu', input_shape=input))\n",
    "    model.add(MaxPool2D(3, 3))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', input_shape=input))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', input_shape=input))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', input_shape=input))\n",
    "    model.add(MaxPool2D(3, 3))\n",
    "    model.add(Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation=\"sigmoid\"))\n",
    "    model.add(tf.keras.layers.Dense(5, activation=\"softmax\"))\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer= \"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_hp(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    filter_size =  hp.Int(\"filter_0\", 2,6)\n",
    "    model.add(Conv2D(hp.Int(\"conv_0_units\", 4, 64, 16), (filter_size, filter_size), activation='relu', input_shape=input))\n",
    "    pool_size =  hp.Int(\"pool_\" + str(1), 1, 4)\n",
    "    model.add(MaxPool2D(pool_size, pool_size))\n",
    "    for i in range(hp.Int(\"n_layers\", 1,3)):\n",
    "        filter_size =  hp.Int(\"filter_\" + str(i + 1), 2, 5)\n",
    "        model.add(Conv2D(hp.Int(\"conv_\" + str(i) + \"_units\", 4, 64, 16), (filter_size, filter_size), activation='relu', input_shape=input))\n",
    "        pool_size =  hp.Int(\"pool_\" + str(i + 1), 1, 3)\n",
    "        model.add(MaxPool2D(pool_size, pool_size))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    for i in range(hp.Int(\"n_layers\", 1,2)):\n",
    "        fc =  hp.Int(\"fc\", 4, 128, 16)\n",
    "        model.add(tf.keras.layers.Dense(fc, activation=\"sigmoid\"))\n",
    "    model.add(tf.keras.layers.Dense(5, activation=\"softmax\"))\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer= \"Adam\", metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criaçao de modelo com transfer learning VGG16 (precisa de input RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tl_model():\n",
    "    p_model = tf.keras.applications.vgg16.VGG16(input_shape = input, weights = \"imagenet\", include_top=False)\n",
    "    p_model.trainable = False\n",
    "    inputs = p_model.input\n",
    "    x = Conv2D(32, 3, activation=\"relu\")(inputs)\n",
    "    x = MaxPool2D((4,4))(x)\n",
    "    x = Conv2D(32, 3, activation=\"relu\")(x)\n",
    "    x = Conv2D(32, 3, activation=\"relu\")(x)\n",
    "    x = Flatten()(p_model.output)\n",
    "    x = Dense(5, activation=\"relu\")(x)\n",
    "    model = Model(inputs, x)\n",
    "    model.summary()\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer= \"Adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "create_tl_model().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit para data generator (mais lento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = create_model()\n",
    "cnn.fit(train_images, validation_data= val_image,epochs = 10, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit para modelo com array (mais rápido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = create_model()\n",
    "print(train_set.shape)\n",
    "cnn.fit(train_set, train_labels, validation_data = (val_set, val_labels), batch_size = 4, epochs = 15, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    create_model_hp,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=200,  # how many model variations to test?\n",
    "    executions_per_trial=3,  # how many trials per variation? (same model could perform differently)\n",
    "    directory=\"Test_150_150\")\n",
    "\n",
    "\n",
    "tuner.search(train_set[:],\n",
    "             train_labels[:], batch_size=16,\n",
    "             verbose=1, # just slapping this here bc jupyter notebook. The console out was getting messy.\n",
    "             epochs=30,\n",
    "             callbacks=[tf.keras.callbacks.EarlyStopping('accuracy', patience=4)],  # if you have callbacks like tensorboard, they go here.\n",
    "             validation_data=(test_set[:], test_labels[:]))\n",
    "\n",
    "\n",
    "tuner.results_summary()\n",
    "model = tuner.get_best_models()[0]\n",
    "model.save(\"good_model_150_150.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "train_path = \"D:/Tese/Projeto BALCAT/Imagens/NIST CARTS/NIST Cartridges/Train\"\n",
    "labels = os.listdir(train_path)\n",
    "\n",
    "def triplet_generator(batch_size=64):\n",
    "    classes = labels\n",
    "    while True:\n",
    "        a = []\n",
    "        p = []\n",
    "        n = []\n",
    "        for x in range(batch_size):\n",
    "            p_n = random.sample(list(classes), 2)\n",
    "            pos = p_n[0]\n",
    "            neg = p_n[1]\n",
    "            #Select anchor and positive sample \n",
    "            positives = random.sample(list(train_set[s_train_labels == pos]), 2)\n",
    "            #Select negative sample \n",
    "            negative = random.choice(list(train_set[s_train_labels == neg]))\n",
    "            a.append(positives[0])\n",
    "            p.append(positives[1])\n",
    "            n.append(negative)\n",
    "        yield ([np.array(a),np.array(p),np.array(n)], np.zeros((batch_size,1)).astype(\"float32\"))\n",
    "\n",
    "def triplet_generator_test(batch_size=100):\n",
    "    classes = labels\n",
    "    a = []\n",
    "    p = []\n",
    "    n = []\n",
    "    for x in range(batch_size):\n",
    "        p_n = random.sample(list(classes), 2)\n",
    "        pos = p_n[0]\n",
    "        neg = p_n[1]\n",
    "        #Select anchor and positive sample \n",
    "        positives = random.sample(list(test_set[s_test_labels == pos]), 2)\n",
    "        #Select negative sample \n",
    "        negative = random.choice(list(test_set[s_test_labels == neg]))\n",
    "        a.append(positives[0])\n",
    "        p.append(positives[1])\n",
    "        n.append(negative)\n",
    "    return ([np.array(a),np.array(p),np.array(n)], np.zeros((batch_size,1)).astype(\"float32\"))\n",
    "\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    anchor_out = y_pred[:,0:50] \n",
    "    positive_out = y_pred[:,50:100] \n",
    "    negative_out = y_pred[:,100:150] \n",
    "\n",
    "    pos_dist = K.sum(K.abs(anchor_out - positive_out), axis = 1)\n",
    "    neg_dist = K.sum(K.abs(anchor_out - negative_out), axis = 1)\n",
    "\n",
    "    probs = K.softmax([pos_dist, neg_dist], axis = 0)\n",
    "\n",
    "    return K.mean(K.abs(probs[0]) + K.abs(1.0 - probs[1]))\n",
    "\n",
    "\n",
    "\n",
    "def accuracy():\n",
    "    correct = 0\n",
    "    val_coords =  trip_model.layers[3].predict(np.array(val_set))\n",
    "    test_coords = trip_model.layers[3].predict(np.array(test_set))\n",
    "    for i,l in zip(val_coords, s_val_labels):\n",
    "        distances = []\n",
    "        coord = i\n",
    "        for j in labels:\n",
    "            model_imgs = list(test_coords[s_test_labels == j])\n",
    "            dist = 0\n",
    "            for gun_img in model_imgs:\n",
    "                dist += np.linalg.norm(coord - gun_img)\n",
    "            dist = dist/len(model_imgs)\n",
    "            distances.append(dist)\n",
    "        print(distances)\n",
    "        label_p = distances.index(min(distances))\n",
    "        label_a = list(labels).index(l)\n",
    "\n",
    "        if(label_a==label_p): \n",
    "            correct+=1\n",
    "    \n",
    "    return correct/len(val_coords)\n",
    "\n",
    "inputs = Input(input)\n",
    "x = Conv2D(32, (3,3), activation=\"relu\")(inputs)\n",
    "x = MaxPool2D((3,3))(x)\n",
    "x = Conv2D(32, (3,3), activation=\"relu\")(x)\n",
    "x = Conv2D(32, (3,3), activation=\"relu\")(x)\n",
    "x = Conv2D(32, (3,3), activation=\"relu\")(x)\n",
    "x = MaxPool2D((3,3))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "\n",
    "model = Model(inputs, x)\n",
    "model.summary()\n",
    "\n",
    "trip_a = Input(input) \n",
    "trip_p = Input(input) \n",
    "trip_n = Input(input) \n",
    "\n",
    "trip_out = Concatenate()([model(trip_a), model(trip_p), model(trip_n)])\n",
    "trip_model = Model([trip_a, trip_p, trip_n], trip_out)\n",
    "\n",
    "trip_model.summary()\n",
    "\n",
    "trip_model.compile(loss=triplet_loss, optimizer=\"adam\")\n",
    "\n",
    "testing = triplet_generator_test(100)\n",
    "\n",
    "trip_model.fit(triplet_generator(), validation_data = testing, steps_per_epoch = 100, epochs = 3)\n",
    "\n",
    "accuracy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "test_lab = label_encoder.fit_transform(s_test_labels)\n",
    "\n",
    "embbedings=trip_model.layers[3].predict(test_set, verbose = 1)\n",
    "embbedings_t=trip_model.layers[3].predict(train_set, verbose = 1)\n",
    "\n",
    "embbedings_reduced = umap.UMAP(n_neighbors = 15, min_dist = 0.3, metric=\"correlation\").fit_transform(embbedings)\n",
    "embbedings_reduced_t = umap.UMAP(n_neighbors = 15, min_dist = 0.3, metric=\"correlation\").fit_transform(embbedings_t)\n",
    "\n",
    "plt.scatter(embbedings_reduced[:,0], embbedings_reduced[:,1], c=test_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = preprocessing.LabelEncoder()\n",
    "test_lab = label_encoder.fit_transform(s_train_labels)\n",
    "\n",
    "embbedings=trip_model.layers[3].predict(test_set, verbose = 1)\n",
    "embbedings_t=trip_model.layers[3].predict(train_set, verbose = 1)\n",
    "\n",
    "embbedings_reduced = umap.UMAP(n_neighbors = 15, min_dist = 0.3, metric=\"correlation\").fit_transform(embbedings)\n",
    "embbedings_reduced_t = umap.UMAP(n_neighbors = 15, min_dist = 0.3, metric=\"correlation\").fit_transform(embbedings_t)\n",
    "\n",
    "plt.scatter(embbedings_reduced_t[:,0], embbedings_reduced_t[:,1], c=test_lab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12880/3101231822.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;31m# train_pairs, train_label_pairs = pair_generator(train_set, s_train_labels, batch_size=1500)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0mtest_pairs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_label_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpair_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_test_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;31m# print(train_pairs.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_set' is not defined"
     ]
    }
   ],
   "source": [
    "def pair_generator(img_list, label_list, batch_size=100):\n",
    "    classes = labels\n",
    "    a = []\n",
    "    p = []\n",
    "    n = []\n",
    "    for x in range(int(batch_size/2)):\n",
    "        p_n = random.sample(list(classes), 2)\n",
    "        pos = p_n[0]\n",
    "        neg = p_n[1]\n",
    "        #Select anchor and positive sample \n",
    "        positives = random.sample(list(img_list[label_list == pos]), 2)\n",
    "        #Select negative sample \n",
    "        negative = random.choice(list(img_list[label_list == neg]))\n",
    "        a.append(positives[0])\n",
    "        p.append(positives[1])\n",
    "        n.append(negative)\n",
    "    \n",
    "    pairs = [[x, y] for x, y in zip(a, p)]\n",
    "    [pairs.append([y, x]) for x, y in zip(a, n)]\n",
    "    print(pairs)\n",
    "    pairs_labels = list(np.ones((len(p))))\n",
    "    pairs_labels.extend(np.zeros((len(n))))\n",
    "    pairs, pairs_labels= shuffle(pairs, pairs_labels)\n",
    "    \n",
    "    return np.asarray(pairs), np.asarray(pairs_labels, dtype=np.int32)\n",
    "\n",
    "\n",
    "\n",
    "def batch_pair_generator(batch_size=32):\n",
    "    classes = labels\n",
    "    while True:\n",
    "        a = []\n",
    "        p = []\n",
    "        n = []\n",
    "        pairs=[]\n",
    "        for x in range(int(batch_size/2)):\n",
    "            p_n = random.sample(list(classes), 2)\n",
    "            pos = p_n[0]\n",
    "            neg = p_n[1]\n",
    "            #Select anchor and positive sample \n",
    "            positives = random.sample(list(train_set[s_train_labels == pos]), 2)\n",
    "            #Select negative sample \n",
    "            negative = random.choice(list(train_set[s_train_labels == neg]))\n",
    "            a.append(positives[0])\n",
    "            p.append(positives[1])\n",
    "            n.append(negative)\n",
    "        \n",
    "        pairs = [[x, y] for x, y in zip(a, p)]\n",
    "        [pairs.append([y, x]) for x, y in zip(a, n)]\n",
    "        pairs_labels = list(np.ones((len(p))))\n",
    "        pairs_labels.extend(np.zeros((len(n))))\n",
    "        pairs, pairs_labels= shuffle(pairs, pairs_labels)\n",
    "        pairs, pairs_labels = np.array(pairs), np.array(pairs_labels)\n",
    "        yield ([pairs[:,0], pairs[:,1]],pairs_labels[:])\n",
    "\n",
    "# train_pairs, train_label_pairs = pair_generator(train_set, s_train_labels, batch_size=1500)\n",
    "test_pairs, test_label_pairs = pair_generator(test_set, s_test_labels, batch_size=200)\n",
    "# print(train_pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 06m 50s]\n",
      "val_accuracy: 0.7350000143051147\n",
      "\n",
      "Best val_accuracy So Far: 0.7549999952316284\n",
      "Total elapsed time: 00h 34m 16s\n",
      "\n",
      "Search: Running Trial #6\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "filter_0          |6                 |6                 \n",
      "conv_0_units      |36                |20                \n",
      "n_layers          |2                 |1                 \n",
      "filter_1          |1                 |3                 \n",
      "pool_1            |1                 |2                 \n",
      "filter_2          |2                 |1                 \n",
      "conv_1_units      |52                |52                \n",
      "pool_2            |2                 |2                 \n",
      "filter_3          |3                 |2                 \n",
      "conv_2_units      |20                |20                \n",
      "pool_3            |1                 |2                 \n",
      "\n",
      "Epoch 1/15\n",
      "20/20 [==============================] - 26s 1s/step - loss: 0.6825 - accuracy: 0.5373 - val_loss: 0.6685 - val_accuracy: 0.6350\n",
      "Epoch 2/15\n",
      "20/20 [==============================] - 26s 1s/step - loss: 0.6611 - accuracy: 0.6029 - val_loss: 0.6530 - val_accuracy: 0.6700\n",
      "Epoch 3/15\n",
      "20/20 [==============================] - 26s 1s/step - loss: 0.6446 - accuracy: 0.6187 - val_loss: 0.6419 - val_accuracy: 0.6600\n",
      "Epoch 4/15\n",
      "20/20 [==============================] - 26s 1s/step - loss: 0.6306 - accuracy: 0.6396 - val_loss: 0.6355 - val_accuracy: 0.6900\n",
      "Epoch 5/15\n",
      "20/20 [==============================] - 27s 1s/step - loss: 0.6187 - accuracy: 0.6500 - val_loss: 0.6294 - val_accuracy: 0.6950\n",
      "Epoch 6/15\n",
      "20/20 [==============================] - 28s 1s/step - loss: 0.6012 - accuracy: 0.6734 - val_loss: 0.6176 - val_accuracy: 0.7100\n",
      "Epoch 7/15\n",
      "20/20 [==============================] - 27s 1s/step - loss: 0.5871 - accuracy: 0.6793 - val_loss: 0.6110 - val_accuracy: 0.7100\n",
      "Epoch 8/15\n",
      "20/20 [==============================] - 30s 2s/step - loss: 0.5719 - accuracy: 0.7053 - val_loss: 0.6004 - val_accuracy: 0.7150\n",
      "Epoch 9/15\n",
      "20/20 [==============================] - 30s 2s/step - loss: 0.5696 - accuracy: 0.6998 - val_loss: 0.5969 - val_accuracy: 0.7150\n",
      "Epoch 10/15\n",
      "12/20 [=================>............] - ETA: 12s - loss: 0.5513 - accuracy: 0.7080"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19360/1791665353.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    106\u001b[0m              \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m              \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# if you have callbacks like tensorboard, they go here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m              validation_data=([test_pairs[:,0], test_pairs[:,1]], test_label_pairs[:]))\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF\\lib\\site-packages\\keras_tuner\\engine\\multi_execution_tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"callbacks\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopied_fit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"min\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[1;34m(self, trial, fit_args, fit_kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \"\"\"\n\u001b[0;32m    146\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1097\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1098\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1100\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\.conda\\envs\\TF\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n",
    "    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n",
    "    \n",
    "def get_siamese_model(hp):\n",
    "    #two input images \n",
    "    left_input = Input(input)\n",
    "    right_input = Input(input)\n",
    "    \n",
    "    # Convolutional Neural Network to extract feature vector\n",
    "    model = Sequential()\n",
    "    \n",
    "    filter_size =  hp.Int(\"filter_0\", 2,6)\n",
    "    model.add(Conv2D(hp.Int(\"conv_0_units\", 4, 64, 16), (filter_size, filter_size), activation='relu', input_shape=input))\n",
    "    model.add(MaxPool2D(3, 3))\n",
    "    for i in range(hp.Int(\"n_layers\", 0,3)):\n",
    "        filter_size =  hp.Int(\"filter_\" + str(i + 1), 1, 6)\n",
    "        model.add(Conv2D(hp.Int(\"conv_\" + str(i) + \"_units\", 4, 64, 16), (filter_size, filter_size), activation='relu'))\n",
    "        pool_size =  hp.Int(\"pool_\" + str(i + 1), 1, 3)\n",
    "        model.add(MaxPool2D(pool_size, pool_size))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation=\"sigmoid\"))\n",
    "\n",
    "    # feature vectors for each image of the pair\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "\n",
    "\n",
    "    # absolute difference between the feature vectors\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    #L1_distance = euclidean_distance([encoded_l, encoded_r])\n",
    "    \n",
    "    # sigmoid layer to predict similarity \n",
    "    prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
    "    \n",
    "    # connect inputs with outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    siamese_net.compile(loss = \"binary_crossentropy\", optimizer= \"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "    return siamese_net\n",
    "\n",
    "def get_siamese_model_tl(hp):\n",
    "    #two input images \n",
    "    left_input = Input(input)\n",
    "    right_input = Input(input)\n",
    "    \n",
    "    # Convolutional Neural Network to extract feature vector\n",
    "    model = Sequential()\n",
    "\n",
    "    p_model = tf.keras.applications.vgg16.VGG16(input_shape = input, weights = \"imagenet\", include_top=False)\n",
    "    p_model.trainable = False\n",
    "    inputs = p_model.input\n",
    "\n",
    "    filter_size =  hp.Int(\"filter_0\", 2,4)\n",
    "    x = Conv2D(hp.Int(\"conv_0_units\", 4, 64, 16), (filter_size, filter_size), activation='relu')(inputs)\n",
    "    x = MaxPool2D(2, 2)(x)\n",
    "    for i in range(hp.Int(\"n_layers\", 0,1)):\n",
    "        filter_size =  hp.Int(\"filter_\" + str(i + 1), 1, 3)\n",
    "        x = Conv2D(hp.Int(\"conv_\" + str(i) + \"_units\", 4, 64, 16), (filter_size, filter_size), activation='relu')(x)\n",
    "        pool_size =  hp.Int(\"pool_\" + str(i + 1), 1, 2)\n",
    "        x = MaxPool2D(pool_size, pool_size)(x)\n",
    "\n",
    "    x = Flatten()(p_model.output)\n",
    "    x = tf.keras.layers.Dense(128, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs, x)\n",
    "\n",
    "\n",
    "    # feature vectors for each image of the pair\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "\n",
    "\n",
    "    # absolute difference between the feature vectors\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    #L1_distance = euclidean_distance([encoded_l, encoded_r])\n",
    "    \n",
    "    # sigmoid layer to predict similarity \n",
    "    prediction = Dense(1,activation='sigmoid')(L1_distance)\n",
    "    \n",
    "    # connect inputs with outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    siamese_net.compile(loss = \"binary_crossentropy\", optimizer= \"Adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "    return siamese_net\n",
    "\n",
    "\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    get_siamese_model_tl,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=30,  # how many model variations to test?\n",
    "    executions_per_trial=1,  # how many trials per variation? (same model could perform differently)\n",
    "    directory=\"50_50\")\n",
    "\n",
    "\n",
    "tuner.search(batch_pair_generator(), steps_per_epoch = 20,\n",
    "             verbose=1, # just slapping this here bc jupyter notebook. The console out was getting messy.\n",
    "             epochs=15,\n",
    "             callbacks=[tf.keras.callbacks.EarlyStopping('accuracy', patience=4)],  # if you have callbacks like tensorboard, they go here.\n",
    "             validation_data=([test_pairs[:,0], test_pairs[:,1]], test_label_pairs[:]))\n",
    "\n",
    "\n",
    "tuner.results_summary()\n",
    "model = tuner.get_best_models()[0]\n",
    "model.save(\"good_model_50_50.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy using distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in 2_Test_150_150\\untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filter_0: 5\n",
      "conv_0_units: 20\n",
      "n_layers: 3\n",
      "filter_1: 2\n",
      "pool_1: 2\n",
      "filter_2: 3\n",
      "conv_1_units: 52\n",
      "pool_2: 1\n",
      "filter_3: 1\n",
      "conv_2_units: 36\n",
      "pool_3: 2\n",
      "Score: 0.8550000190734863\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filter_0: 4\n",
      "conv_0_units: 20\n",
      "n_layers: 3\n",
      "filter_1: 2\n",
      "pool_1: 1\n",
      "filter_2: 4\n",
      "conv_1_units: 52\n",
      "pool_2: 2\n",
      "filter_3: 4\n",
      "conv_2_units: 52\n",
      "pool_3: 1\n",
      "Score: 0.8299999833106995\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filter_0: 3\n",
      "conv_0_units: 52\n",
      "n_layers: 2\n",
      "filter_1: 5\n",
      "pool_1: 2\n",
      "filter_2: 4\n",
      "conv_1_units: 36\n",
      "pool_2: 2\n",
      "filter_3: 4\n",
      "conv_2_units: 20\n",
      "pool_3: 1\n",
      "Score: 0.8100000023841858\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filter_0: 4\n",
      "conv_0_units: 20\n",
      "n_layers: 1\n",
      "filter_1: 5\n",
      "pool_1: 1\n",
      "filter_2: 4\n",
      "conv_1_units: 36\n",
      "pool_2: 1\n",
      "filter_3: 2\n",
      "conv_2_units: 20\n",
      "pool_3: 2\n",
      "Score: 0.8100000023841858\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filter_0: 5\n",
      "conv_0_units: 4\n",
      "n_layers: 2\n",
      "filter_1: 3\n",
      "pool_1: 3\n",
      "filter_2: 1\n",
      "conv_1_units: 52\n",
      "pool_2: 1\n",
      "filter_3: 4\n",
      "conv_2_units: 4\n",
      "pool_3: 1\n",
      "Score: 0.8050000071525574\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filter_0: 6\n",
      "conv_0_units: 52\n",
      "n_layers: 1\n",
      "filter_1: 6\n",
      "pool_1: 2\n",
      "filter_2: 2\n",
      "conv_1_units: 4\n",
      "pool_2: 2\n",
      "filter_3: 6\n",
      "conv_2_units: 36\n",
      "pool_3: 1\n",
      "Score: 0.800000011920929\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filter_0: 4\n",
      "conv_0_units: 36\n",
      "n_layers: 1\n",
      "filter_1: 4\n",
      "pool_1: 3\n",
      "filter_2: 2\n",
      "conv_1_units: 52\n",
      "pool_2: 1\n",
      "filter_3: 2\n",
      "conv_2_units: 20\n",
      "pool_3: 2\n",
      "Score: 0.800000011920929\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filter_0: 4\n",
      "conv_0_units: 20\n",
      "n_layers: 3\n",
      "filter_1: 5\n",
      "pool_1: 1\n",
      "filter_2: 2\n",
      "conv_1_units: 20\n",
      "pool_2: 3\n",
      "filter_3: 5\n",
      "conv_2_units: 36\n",
      "pool_3: 2\n",
      "Score: 0.7950000166893005\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filter_0: 4\n",
      "conv_0_units: 52\n",
      "n_layers: 2\n",
      "filter_1: 4\n",
      "pool_1: 1\n",
      "filter_2: 5\n",
      "conv_1_units: 36\n",
      "pool_2: 1\n",
      "filter_3: 2\n",
      "conv_2_units: 36\n",
      "pool_3: 2\n",
      "Score: 0.7950000166893005\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "filter_0: 5\n",
      "conv_0_units: 52\n",
      "n_layers: 1\n",
      "filter_1: 3\n",
      "pool_1: 1\n",
      "filter_2: 2\n",
      "conv_1_units: 20\n",
      "pool_2: 3\n",
      "filter_3: 6\n",
      "conv_2_units: 20\n",
      "pool_3: 1\n",
      "Score: 0.7900000214576721\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "[4.497098198044772, 5.320213078127967, 3.693711429172092, 2.9954093297322593, 3.605018373992708]\n",
      "[3.1838192865673434, 6.51830759578281, 5.13557708925671, 4.414920409520467, 5.205529914961921]\n",
      "[6.514966980885651, 3.281254519356622, 4.713561432891422, 5.407950626479255, 4.704344632890489]\n",
      "[5.292285199892723, 4.297965151733822, 3.192581557565265, 3.9209498551156785, 3.2915935973326365]\n",
      "[4.762061249738359, 4.6216774053043785, 2.8249044477939607, 3.403288357787662, 2.9578522715303635]\n",
      "[5.163388710237492, 4.763577654626634, 2.8682012650701734, 3.7090170807308622, 3.1191231966018678]\n",
      "[3.2414583001433117, 6.476942933930291, 5.099261967341105, 4.4191213011741635, 5.183914277288649]\n",
      "[5.066627112485595, 4.876543784141541, 2.8689327617486318, 3.4381545219156475, 2.9260001328256395]\n",
      "[4.823423574199785, 4.781391831239064, 3.1856832848654855, 3.9696092181735567, 3.390452211432987]\n",
      "[4.62719763941684, 5.420785326427883, 3.727454431851705, 3.2785980463027955, 3.740872781806522]\n",
      "[5.090127321959889, 5.068077849017249, 3.395204985472891, 4.071542106734382, 3.4779568400647904]\n",
      "[5.422192530443439, 4.572855938805474, 3.2887094775835672, 4.182480548487769, 3.484952951139874]\n",
      "[4.7279883306578725, 4.784978863928053, 3.0625808152887557, 3.3463585575421653, 3.123374821080102]\n",
      "[3.518716099572047, 5.917378211021424, 4.63197294473648, 3.856466677453783, 4.6815385818481445]\n",
      "[5.135194293523239, 4.6503778682814705, 2.9389464053842755, 3.559609815147188, 3.013705215189192]\n",
      "[5.791953764392831, 3.802688667509291, 4.636846432420943, 5.350958893034194, 4.678898641135958]\n",
      "[6.4297693489635055, 2.9759262952539656, 4.806456029415131, 5.43842040432824, 4.756580901808209]\n",
      "[4.905542581094861, 4.934615450435214, 3.2363925053013696, 3.933632495668199, 3.3652565346823797]\n",
      "[3.363122282055138, 6.686617321438259, 5.589059212472704, 4.941501601537069, 5.648538480864631]\n",
      "[5.099420538056369, 4.803711439503564, 2.8593000133832294, 3.4065545903311834, 2.8588661422332127]\n",
      "[5.146926160586083, 4.957954947153727, 3.722084810998705, 4.051211475001441, 3.602524596452713]\n",
      "[4.591029479678741, 5.119541540410784, 4.027766978078418, 3.447504797246721, 3.8685755517747666]\n",
      "[5.286092729891761, 4.990214088228014, 4.390054743157493, 4.107854000727335, 4.122631041208903]\n",
      "[5.0367264491690085, 4.838217576344808, 3.4491133630275725, 4.326947225464715, 3.653498325745265]\n",
      "[5.154831482192217, 4.703490897019704, 2.9803471095032164, 3.5799444708559247, 3.0077433976862165]\n",
      "[4.810110425544997, 4.925023023287455, 3.011229385270013, 3.134154498577118, 2.9525666303104825]\n",
      "[4.824983171150509, 5.058796200487349, 3.036575276984109, 3.124372363090515, 3.039043730828497]\n",
      "[3.643503164840957, 6.28352071709103, 5.147722832361857, 4.862141001224518, 5.272522326310476]\n",
      "[5.389341518703827, 4.1578777266873255, 3.8709428932931687, 4.419057924217648, 3.7417720006571877]\n",
      "[4.19172143464708, 5.126670938067966, 3.3281921757592094, 3.3469399054845175, 3.380955980883704]\n",
      "[3.277570855819573, 6.599027061462403, 5.461778510941399, 4.670744480027093, 5.497326233651903]\n",
      "[5.6836518783353815, 3.824406909280353, 4.395836693710751, 5.100464602311452, 4.4022022075123255]\n",
      "[6.320270096514858, 3.063293421268463, 4.599989910920461, 5.260934199227227, 4.595459373792012]\n",
      "[5.011005155110763, 4.928450466526879, 2.932085194190343, 3.4871629006332823, 3.0740678820345138]\n",
      "[5.786354361280883, 3.7589541832605997, 3.9182980087068344, 4.551350284947289, 3.904141096274058]\n",
      "[6.131536144321248, 3.126795537604226, 4.475723032156626, 5.018320568402609, 4.401771799723307]\n",
      "[3.3397136663986466, 6.338521522945828, 5.0114236460791695, 4.65599126153522, 5.161707496643066]\n",
      "[5.221902967172827, 4.412396759457058, 3.053297723001904, 4.017299894491831, 3.2763875590430365]\n",
      "[3.9330391722210383, 5.988487492667304, 4.791628117031522, 3.7383188724517824, 4.683580034308964]\n",
      "[3.248043196349494, 6.679961752891541, 5.542644569608901, 4.798588546117147, 5.602779984474182]\n",
      "[6.6032157294494285, 3.3020564887258743, 4.827925082047781, 5.540655970573425, 4.806073433823055]\n",
      "[4.842491553328132, 5.059107828140259, 3.332825438181559, 3.3055016160011292, 3.2901185545656415]\n",
      "[5.361467312958281, 4.642383677429623, 3.0212475856145224, 4.022913061247932, 3.304798048734665]\n",
      "[3.2252513392496915, 6.598622131347656, 5.471370895703633, 4.777515235212114, 5.552787856260935]\n",
      "[3.98207268607145, 5.998534093962776, 4.4541329807705345, 3.659619070423974, 4.424006554815504]\n",
      "[3.944906372134968, 5.4007156822416515, 4.254557926124996, 3.733472955889172, 4.196819513373905]\n",
      "[5.267921673376008, 4.51340274280972, 3.269317329592175, 3.7449330515331694, 3.204980836311976]\n",
      "[5.103381709190412, 4.753930457433065, 2.7935571445359124, 3.5263808886210124, 2.941563642024994]\n",
      "[5.751353569623441, 3.708785483572218, 3.680410236782498, 4.686995621522268, 3.9285433265897964]\n",
      "[6.236224306505279, 2.8804674731360542, 4.363920891284943, 5.053887640105353, 4.313279988368352]\n",
      "[5.16500879681043, 4.782347673839993, 2.8867199242115023, 3.6666464375125036, 3.059462781747182]\n",
      "[5.5835951492611295, 4.306488745742374, 3.300470758808984, 4.402513133154975, 3.599831559260686]\n",
      "[4.409708253407882, 4.904401360617744, 3.1265331082873873, 3.673058557510376, 3.3046754823790656]\n",
      "[4.870502472597327, 4.7814724577797785, 2.8961716552575427, 3.269916511244244, 2.8552190211084154]\n",
      "[5.087972740669035, 5.009124876393212, 3.027227775255839, 3.5171287907494437, 3.0572930779722003]\n",
      "[5.049666051810744, 4.91060372988383, 3.017871161633068, 3.5203241460853154, 3.17170319226053]\n",
      "[3.153320505794159, 6.586019600762262, 5.363987992869483, 4.639813427130381, 5.445796635415819]\n",
      "[3.762929351989832, 6.237106058332655, 5.0942047701941595, 4.888446627722846, 5.196926164627075]\n",
      "[3.453839702121282, 6.71007968849606, 5.699008401234945, 4.8927697274419994, 5.711239544550578]\n",
      "[3.267932571933768, 6.540524207221138, 5.282863631513384, 4.489268554581536, 5.313266928990682]\n",
      "[5.446784688928033, 4.060241835647159, 3.2700214372740852, 4.101308635870615, 3.4343286209636266]\n",
      "[5.911770500032242, 3.8526404566235013, 4.1720435566372345, 4.71791457467609, 4.116618798838721]\n",
      "[4.4960013823320635, 5.0755327900250755, 3.3766495850351124, 2.861856285730998, 3.18768250213729]\n",
      "[4.395324110311304, 5.328193573156993, 3.792336042722066, 2.8204935994413165, 3.6472839183277554]\n",
      "[5.080976620232318, 4.6850538889567055, 3.1781993958685133, 3.7984778708881803, 3.2637696193324195]\n",
      "[3.6863518589634006, 5.339175022972954, 3.6944546461105348, 2.9022761285305023, 3.6759983155462477]\n",
      "[5.172906701847658, 4.958357192410363, 3.621332527531518, 4.422771738635169, 3.7720347656144035]\n",
      "[3.735681438176645, 5.768631794717577, 4.2429178833961485, 4.092926586998834, 4.352423368559943]\n",
      "[5.099642081449261, 4.907183339860704, 3.0629125972588858, 3.4790842645698126, 3.099795213010576]\n",
      "[3.3720259471128218, 6.589589187833998, 5.401176015535991, 4.741397179497612, 5.475528523657057]\n",
      "[5.976946306767437, 3.5941560043228997, 4.948567716280619, 5.509851225217184, 4.886052037609948]\n",
      "[4.616582649575788, 5.907612235016293, 4.4687232818868425, 3.4051740606625875, 4.280474330319299]\n",
      "[6.333279978757524, 3.1928011132611167, 4.766166232691871, 5.342212989595201, 4.696207610766093]\n",
      "[5.409688569731632, 5.296649834844801, 4.689280982149972, 4.337339448928833, 4.415986516740587]\n",
      "[5.079295285004007, 4.936357288890415, 3.307853031158447, 4.077255336443583, 3.464675924513075]\n",
      "[3.481630641188325, 5.920284186469184, 4.401371460490757, 3.595759094423718, 4.4463655299610565]\n",
      "[4.875755279751147, 4.983325884077284, 3.2160148481527964, 3.24339338209894, 3.171575215790007]\n",
      "[4.992119127074204, 4.877639558580187, 2.9734253916475506, 3.2847053011258445, 2.828205812308523]\n",
      "[3.218572362668097, 6.640456332100762, 5.356228795316484, 4.49456601275338, 5.377853955162896]\n",
      "[3.3994984397780423, 5.892557374636332, 4.731763101948632, 4.18182450665368, 4.740161342091031]\n",
      "[3.7154031533979426, 6.608344152238634, 5.529769203397962, 4.855408549308777, 5.61385932498508]\n",
      "[4.630177002168645, 5.761566828356849, 4.379812822077009, 3.340114007393519, 4.182933518621657]\n",
      "[3.2589706054515086, 6.5013298802905615, 5.335699854956733, 4.749193750487434, 5.394868111610412]\n",
      "[5.6797914424185025, 3.7671568115552265, 4.2523614320490095, 5.048069389661153, 4.3073786444134186]\n",
      "[4.5107923429564565, 5.185032337241703, 3.4614186419381037, 2.7262040747536553, 3.280922320816252]\n",
      "[5.820534469044141, 3.5010192897584704, 3.960710491074456, 4.577326464653015, 3.9156543996598985]\n",
      "[3.327751767837395, 6.761538132031759, 5.660124892658658, 4.9615559577941895, 5.73545404540168]\n",
      "[4.511104438264491, 5.193360091580285, 3.4155487623479632, 3.232525509595871, 3.360952677991655]\n",
      "[4.512178058678147, 5.487107639842563, 4.005976880921258, 3.0499511192242306, 3.8638605819808114]\n",
      "[5.043717835582583, 4.917911575900184, 3.0584579951233333, 3.462244341108534, 3.119967605339156]\n",
      "[5.262242596028215, 4.917959119213952, 3.6216072844134435, 4.263857135507796, 3.5700856652524737]\n",
      "[5.005506703409098, 5.0606525116496615, 3.357831045985222, 4.063114224539863, 3.5090827928649055]\n",
      "[5.139984097857933, 5.367213782999251, 4.115573947297202, 3.85350510014428, 3.896860615412394]\n",
      "[5.137909463569943, 4.997096917364332, 3.4167036182350583, 4.230559521251255, 3.592752747403251]\n",
      "[3.2750868595252602, 6.298141151004367, 4.966609782642788, 4.450352084636688, 5.076279966036479]\n",
      "[5.024723471221277, 4.924662498633067, 3.239205739233229, 3.9204045130146876, 3.4438319073783026]\n",
      "[4.228458335844137, 5.645684970749749, 4.247356259822846, 3.075571741660436, 4.0991944392522175]\n",
      "[6.463433130986273, 3.1308981484837, 4.756748803456625, 5.453838019900852, 4.745947764979468]\n",
      "[3.849124578432848, 5.864122210608588, 4.493978095054627, 4.701913189888001, 4.680206387572818]\n",
      "[5.357968739870578, 4.563874472512139, 3.164227936002943, 4.044128771622976, 3.3627913455168406]\n",
      "[3.4278388488090643, 6.383721635076735, 5.28814250893063, 4.3608426478174, 5.242318026224772]\n",
      "[6.256766917341847, 2.9605920864476096, 4.359162019358741, 5.077408374680413, 4.3529584070046745]\n",
      "[6.05694009085833, 3.7666047771771747, 5.028125480810801, 5.592537432246738, 4.951519576708476]\n",
      "[3.3513752168181252, 6.066285702917311, 4.533408770296309, 3.7987578127119277, 4.566306495666504]\n",
      "[4.518692779002216, 5.559679194291433, 4.14994492398368, 3.2211950620015464, 3.9494225806660124]\n",
      "[3.3835202866354903, 6.696985530853271, 5.643070374594794, 4.941219744417403, 5.713207899199592]\n",
      "[5.4991306477347335, 3.735228956407971, 4.018194851610396, 4.781775124867758, 4.049806057082282]\n",
      "[4.599653142320234, 5.091221015983158, 3.3013805905977884, 2.652074076400863, 3.0993380818102096]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7037037037037037"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#model = tf.keras.models.load_model('siamese83_80_80.h5')\n",
    "tuner.results_summary()\n",
    "model = tuner.get_best_models()[0]\n",
    "\n",
    "model = model.get_layer(\"sequential\")\n",
    "\n",
    "model.save(\"85_150_150.h5\")\n",
    "\n",
    "def accuracy():\n",
    "    correct = 0\n",
    "    val_coords =  model.predict(np.array(val_set))\n",
    "    test_coords = model.predict(np.array(train_set))\n",
    "    for i,l in zip(val_coords, s_val_labels):\n",
    "        distances = []\n",
    "        coord = i\n",
    "        for j in labels:\n",
    "            model_imgs = list(test_coords[s_train_labels == j])\n",
    "            dist = 0\n",
    "            for gun_img in model_imgs:\n",
    "                dist += np.linalg.norm(coord - gun_img)\n",
    "            dist = dist/len(model_imgs)\n",
    "            distances.append(dist)\n",
    "        print(distances)\n",
    "        label_p = distances.index(min(distances))\n",
    "        label_a = list(labels).index(l)\n",
    "\n",
    "        if(label_a==label_p): \n",
    "            correct+=1\n",
    "    \n",
    "    return correct/len(val_coords)\n",
    "accuracy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further train the best network\n",
    "model = tf.keras.models.load_model('siamese83_80_80.h5')\n",
    "\n",
    "#Faz reset ao modelo (elimina os pesos)\n",
    "model = tf.keras.models.clone_model(model)\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer= \"ADAM\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "model.fit(batch_pair_generator(), steps_per_epoch = 100,\n",
    "verbose=1, # just slapping this here bc jupyter notebook. The console out was getting messy.\n",
    "            epochs=15,\n",
    "            validation_data=([test_pairs[:,0], test_pairs[:,1]], test_label_pairs[:])\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = model.predict(train_set)\n",
    "X_test = model.predict(test_set)\n",
    "y_train = s_train_labels\n",
    "y_test = s_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest on top of SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8018867924528302\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM on top of SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8113207547169812\n"
     ]
    }
   ],
   "source": [
    "#Import svm model\n",
    "from sklearn import svm\n",
    "\n",
    "#Create a svm Classifier\n",
    "clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN on top SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0 4 1 0 1 1 1 1 1 1 1 4 3 4 4 2 0 3 2 0]\n",
      "Accuracy: 0.9545454545454546\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "#creating labelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Converting string labels into numbers.\n",
    "labels_encoded=le.fit_transform(s_test_labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(model.predict(test_set), labels_encoded, test_size=0.2)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the model using the training sets\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "#Predict Output\n",
    "predicted= knn.predict(X_test) \n",
    "print(predicted)\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 1 2 2 2 0 2 2 4 2 2 3 4 3 1 1 3 0 2 2 2 4 2 3 3 2 0 3 3 0 1 1 2 3 1 0\n",
      " 2 4 0 1 2 2 0 0 4 3 2 3 3 2 2 3 2 3 2 0 0 0 0 3 3 4 4 2 4 3 0 3 0 1 4 1 4\n",
      " 2 4 2 3 0 0 0 4 0 1 4 1 0 3 4 3 2 2 2 2 0 3 4 1 0 3 0 1 1 0 4 0 3 4]\n",
      "Accuracy: 0.6759259259259259\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "#creating labelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Converting string labels into numbers.\n",
    "labels_encoded=le.fit_transform(s_val_labels)\n",
    "\n",
    "X_test = model.predict(val_set)\n",
    "y_test = labels_encoded\n",
    "# \n",
    "#Predict Output\n",
    "predicted= knn.predict(X_test) \n",
    "print(predicted)\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, predicted))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "15af266f599dc5d3438e2abbe598328185b58d1b0436069976965acefbda6c99"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('TBDB': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
